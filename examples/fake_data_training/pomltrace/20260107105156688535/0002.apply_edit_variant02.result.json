{
  "messages": [
    {
      "speaker": "system",
      "content": "Revise the prompt to address ONE critique point clearly and effectively. Preserve all variable names in curly-brackets.\n\nDo not address more than one critique point. Focus on the single most critical issue.\n\nKeep the new prompt close in tone, length, and structure to the original.\n\n# Output Format\n\nReturn only the revised full prompt. Do not include explanations, comparisons, or other text."
    },
    {
      "speaker": "human",
      "content": "### PROMPT\n\nAnswer the following question with only 'yes' or 'no': {question}\n\n### CRITIQUE\n\n- Make the required output exact and machine-validateable. Example instruction to use instead: \"Respond with exactly one token: yes or no. Output must be lowercase, no punctuation, no surrounding whitespace or newline.\" Add a validation regex: ^(yes|no)$.\n\n- Put format/constraints before the question. e.g.:\n  1) \"Respond exactly with 'yes' or 'no' (lowercase, no punctuation).\"\n  2) \"Question: {question}\"\n\n- Show 2–4 few-shot examples that match the exact allowed outputs (no punctuation, no extra words). Example pairs: \"Is a bird a mammal? -> no\" \"Is ice cold? -> yes\".\n\n- Constrain model generation settings to prevent extra text: set temperature = 0 and set max_tokens = 1 (or 2 if the API requires a safety token). Also add a stop sequence (if supported) immediately after the token.\n\n- Define behavior for ambiguous or unanswerable questions. Example rule: \"If the question cannot be answered definitively, output 'no'.\" (or choose a different deterministic fallback) — include that rule explicitly so behavior is deterministic.\n\n- Remove punctuation from the template prompt that may encourage punctuation in the reply (remove trailing colon/quotes). Use plain imperative statements: \"Output exactly: yes or no\" rather than \"...with only 'yes' or 'no':\".\n\n- Add a short automated test suite (concrete tests). Example tests to run: \n  - Input: \"Is a bird a mammal?\" → expected: no\n  - Input: \"Is ice cold?\" → expected: yes\n  - Input: \"Is this question answerable?\" (ambiguous) → expected per chosen fallback (e.g., no)\n  - Validate outputs against the regex and reject otherwise.\n\n- If downstream grader is sensitive to case/punctuation, canonicalize outputs before scoring (trim whitespace, lower-case, strip punctuation) as a secondary safety net and report mismatches.\n\n- If you must allow only two tokens, consider returning numeric tokens instead (0/1) or using an enumerated field in JSON {\"answer\":\"yes\"} to further reduce parsing errors.\n\n- Keep the instruction minimal and imperative; avoid optional language (\"please\", \"only\") that sometimes leads to extra tokens. Example final prompt:\n  \"Output exactly one token: yes or no (lowercase, no punctuation or spaces). Question: {question}\"\n\nThese changes are concrete and testable: update the prompt, set model params (temperature, max_tokens, stop), add few-shot examples, and run the suggested test suite verifying outputs match ^(yes|no)$."
    }
  ]
}